{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C-SWM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using HDF5\n",
    "using Knet\n",
    "using Statistics: mean,std\n",
    "using Random\n",
    "include(\"utils.jl\");\n",
    "import Base: iterate, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Params\n",
    "SAVE_FOLDER = \"./checkpoints\"\n",
    "NUM_STEPS = 1\n",
    "TRAIN_DATASET_PATH = \"/home/cagan/dev/datasets/C-SWM/shapes_train.h5\"\n",
    "BATCH_SIZE = 100\n",
    "SEED = 0\n",
    "NUM_OBJECTS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtrn = buildDataset(TRAIN_DATASET_PATH, true, BATCH_SIZE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  (k, (obs, action, next_obs)) in enumerate(dtrn)\n",
    "    \n",
    "    #Train\n",
    "    println(obs)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C-SWM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderCNNSmall\n",
    "\n",
    "TODO: Add bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct EncoderCNNSmall\n",
    "   \n",
    "    weights\n",
    "    bn_vars\n",
    "    act_fn\n",
    "    act_fn_hid\n",
    "    \n",
    "end\n",
    "\n",
    "function initEncoderCNNSmall(input_dim, hidden_dim, num_objects, act_fn, act_fn_hid)\n",
    "    \n",
    "    weights = Any[param(10,10,3,hidden_dim), param(1,1,hidden_dim,num_objects)]\n",
    "    bn_vars = Any[bnmoments(), atype(bnparams(hidden_dim))]\n",
    "    \n",
    "    return EncoderCNNSmall(weights, bn_vars, act_fn, act_fn_hid)\n",
    "    \n",
    "end\n",
    "\n",
    "function (e_cnn::EncoderCNNSmall)(x)\n",
    "    \n",
    "    #w(w_x,w_y,in_ch,out_ch)\n",
    "    x1 = conv4(e_cnn.weights[1],x; stride = 10)\n",
    "    x2 = e_cnn.act_fn_hid.(batchnorm(x1, e_cnn.bn_vars[1], e_cnn.bn_vars[2]))\n",
    "    y = e_cnn.act_fn.(conv4(e_cnn.weights[2],x2;stride=1))\n",
    "    \n",
    "    return y\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_extractor = initEncoderCNNSmall(3,64,6,sigm,relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = obj_extractor(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm Layer (Taken from: https://github.com/denizyuret/Knet.jl/issues/492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct LayerNorm; a; b; ϵ; end\n",
    "\n",
    "function LayerNorm(dmodel; eps=1e-6)\n",
    "    a = param(dmodel; init=ones)\n",
    "    b = param(dmodel; init=zeros)\n",
    "    return LayerNorm(a, b, eps)\n",
    "end\n",
    "\n",
    "function (l::LayerNorm)(x, o...)\n",
    "    μ = mean(x,dims=1)\n",
    "    σ = std(x,mean=μ,dims=1)\n",
    "    return l.a .* (x .- μ) ./ (σ .+ l.ϵ) .+ l.b                                                         \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct EncoderMLP\n",
    "    \n",
    "    weights\n",
    "    biases\n",
    "    layer_norm\n",
    "    act_fn\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "function initEncoderMLP(input_dim, hidden_dim, output_dim, num_objects, act_fn)\n",
    "    \n",
    "    weights = [param(hidden_dim,input_dim),param(hidden_dim, hidden_dim),param(output_dim,hidden_dim )]\n",
    "    biases =  [param0(hidden_dim), param0(hidden_dim), param0(output_dim)]\n",
    "    layer_norm = LayerNorm(hidden_dim)\n",
    "    \n",
    "    return EncoderMLP(weights, biases, layer_norm, act_fn)\n",
    "    \n",
    "end\n",
    "\n",
    "function (e_mlp::EncoderMLP)(x)\n",
    "    \n",
    "    dims = size(x)\n",
    "    println(dims)\n",
    "    x0 = reshape(x, dims[1]*dims[2], dims[3]*dims[4])\n",
    "    println(size(x0))\n",
    "    x1 = e_mlp.act_fn.(e_mlp.weights[1] * x0 .+ e_mlp.biases[1])\n",
    "    println(size(x1))\n",
    "    x2 = e_mlp.weights[2]* x1 .+ e_mlp.biases[2]\n",
    "    println(size(x2))\n",
    "    x3 = e_mlp.act_fn.(e_mlp.layer_norm(x2))\n",
    "    println(size(x3))\n",
    "    x4 = e_mlp.act_fn.(e_mlp.weights[3]*x3 .+ e_mlp.biases[3])\n",
    "    x5 = reshape(x4, :, dims[3],dims[4])\n",
    "    \n",
    "    return x5\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input dim = 25\n",
    "# Hidden dim = 512\n",
    "# Output dim = 2\n",
    "\n",
    "obj_encoder = initEncoderMLP(25,512,2,5,relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "o = obj_encoder(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct EdgeMLP\n",
    "    \n",
    "    weights\n",
    "    biases\n",
    "    layer_norm\n",
    "    act_fn\n",
    "  \n",
    "end\n",
    "\n",
    "\n",
    "function initEdgeMLP(input_dim, hidden_dim, act_fn)\n",
    "    \n",
    "    weights = [param(hidden_dim,input_dim*2),param(hidden_dim, hidden_dim),param(hidden_dim,hidden_dim)]\n",
    "    biases =  [param0(hidden_dim), param0(hidden_dim), param0(hidden_dim)]    \n",
    "    layer_norm = LayerNorm(hidden_dim)\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function (edge_mlp::EdgeMLP)(x)\n",
    "    \n",
    "    x1 = e_mlp.act_fn.(e_mlp.weights[1] * x .+ e_mlp.biases[1])\n",
    "    println(size(x1))\n",
    "    x2 = e_mlp.weights[2]* x1 .+ e_mlp.biases[2]\n",
    "    println(size(x2))\n",
    "    x3 = e_mlp.act_fn.(e_mlp.layer_norm(x2))\n",
    "    println(size(x3))\n",
    "    x4 = e_mlp.act_fn.(e_mlp.weights[3]*x3 .+ e_mlp.biases[3])\n",
    "    \n",
    "    return x4\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct NodeMLP\n",
    "    \n",
    "    weights\n",
    "    biases\n",
    "    layer_norm\n",
    "    act_fn\n",
    "    \n",
    "end\n",
    "\n",
    "function initNodeMLP(node_input_dim, hidden_dim, act_fn, input_dim)\n",
    "    \n",
    "    weights = [param(hidden_dim,node_input_dim),param(hidden_dim, hidden_dim),param(input_dim,hidden_dim )]\n",
    "    biases =  [param0(hidden_dim), param0(hidden_dim), param0(input_dim)]    \n",
    "    layer_norm = LayerNorm(hidden_dim)\n",
    "    \n",
    "    return NodeMLP(weights, biases, layer_norm, act_fn)\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function (node_mlp::NodeMLP)(x)\n",
    "    \n",
    "    x1 = e_mlp.act_fn.(e_mlp.weights[1] * x .+ e_mlp.biases[1])\n",
    "    println(size(x1))\n",
    "    x2 = e_mlp.weights[2]* x1 .+ e_mlp.biases[2]\n",
    "    println(size(x2))\n",
    "    x3 = e_mlp.act_fn.(e_mlp.layer_norm(x2))\n",
    "    println(size(x3))\n",
    "    x4 = e_mlp.act_fn.(e_mlp.weights[3]*x3 .+ e_mlp.biases[3])\n",
    "    \n",
    "    return\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TransitionGNN\n",
    "    \n",
    "    edge_mlp\n",
    "    node_mlp\n",
    "    \n",
    "    ignore_action\n",
    "    copy_action\n",
    "    action_dim\n",
    "    \n",
    "    edge_list\n",
    "    batch_size\n",
    "    \n",
    "end\n",
    "\n",
    "function initTransitionGNN(input_dim, hidden_dim, action_dim, num_objects, ignore_action, copy_action, act_fn)\n",
    "    \n",
    "    if ignore_action\n",
    "        \n",
    "        action_dim = 0\n",
    "        \n",
    "    end\n",
    "    \n",
    "    #Edge MLP\n",
    "    edge_mlp = initEdgeMLP(input_dim, hidden_dim, act_fn)\n",
    "    node_input_dim = hidden_dim + input_dim + action_dim\n",
    "    \n",
    "    #Node MLP\n",
    "    node_mlp = initNodeMLP(node_input_dim, hidden_dim, act_fn, input_dim)\n",
    "    \n",
    "    edge_list = Any[]\n",
    "    batch_size = 0\n",
    "    \n",
    "    return TransitionGNN(edge_mlp, node_mlp, ignore_action, copy_action, action_dim, edge_list, batch_size)\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function get_edge_list_fully_connected()\n",
    "    \n",
    "end\n",
    "\n",
    "function (t_gnn::TransitionGNN)(x)\n",
    "    \n",
    "    \n",
    "    #Todo forward prop\n",
    "    dimensions = size(x)\n",
    "    batch_size = dimensions[1]\n",
    "    num_nodes = dimensions[2]\n",
    "    \n",
    "    node_attr  = \n",
    "    \n",
    "    if num_nodes > 1\n",
    "        \n",
    "    end\n",
    "    \n",
    "    if !t_gnn.ignore_action\n",
    "        \n",
    "        if t_gnn.copy_action\n",
    "            \n",
    "            action_vec = toOneHot(action_dim, )\n",
    "            \n",
    "        else\n",
    "            \n",
    "            \n",
    "            \n",
    "        end\n",
    "        \n",
    "    end    \n",
    "    \n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
